{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gru_ode_bayes\n",
    "import gru_ode_bayes.data_utils as data_utils\n",
    "import time\n",
    "import tqdm\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from gru_ode_bayes import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gruode(simulation_name,params_dict,device, train_idx, val_idx, test_idx, epoch_max=40):\n",
    "\n",
    "    csv_file_path   = params_dict[\"csv_file_path\"]\n",
    "    pd_df           = pd.read_csv(csv_file_path)\n",
    "    N               = pd_df[\"ID\"].nunique() \n",
    "    \n",
    "\n",
    "    if params_dict[\"lambda\"]==0:\n",
    "        validation = True\n",
    "        val_options = {\"T_val\": params_dict[\"T_val\"], \"max_val_samples\": params_dict[\"max_val_samples\"]}\n",
    "        logger = Logger(f'./Logs/{simulation_name}')\n",
    "\n",
    "    else:\n",
    "        validation = False\n",
    "        val_options = None\n",
    "        logger = Logger(f'./Logs/{simulation_name}')\n",
    "\n",
    "\n",
    "    data_train = data_utils.ODE_Dataset(panda_df=pd_df,\n",
    "                                        idx=train_idx)\n",
    "    data_val   = data_utils.ODE_Dataset(panda_df=pd_df,\n",
    "                                        idx=val_idx,\n",
    "                                        validation = validation,\n",
    "                                        val_options = val_options)\n",
    "    data_test  = data_utils.ODE_Dataset(panda_df=pd_df,\n",
    "                                        idx=test_idx,\n",
    "                                        validation = validation,\n",
    "                                        val_options = val_options)\n",
    "\n",
    "\n",
    "    dl      = DataLoader(dataset=data_train,\n",
    "                         collate_fn=data_utils.custom_collate_fn,\n",
    "                         shuffle=False,\n",
    "                         batch_size=20,\n",
    "                         num_workers=2)\n",
    "    dl_val  = DataLoader(dataset=data_val,\n",
    "                         collate_fn=data_utils.custom_collate_fn,\n",
    "                         shuffle=False,\n",
    "                         batch_size=len(val_idx))\n",
    "    dl_test = DataLoader(dataset=data_test,\n",
    "                         collate_fn=data_utils.custom_collate_fn,\n",
    "                         shuffle=False,\n",
    "                         batch_size=len(test_idx))\n",
    "\n",
    "\n",
    "    params_dict[\"input_size\"]   = data_train.variable_num\n",
    "    params_dict[\"cov_size\"]     = data_train.cov_dim\n",
    "\n",
    "  \n",
    "\n",
    "    train_folder = \"./../trained_models/\"\n",
    "    if not os.path.exists(train_folder):\n",
    "        os.makedirs(train_folder)\n",
    "    np.save(train_folder + f\"{simulation_name}_params.npy\", params_dict)\n",
    "\n",
    "    nnfwobj = gru_ode_bayes.NNFOwithBayesianJumps(input_size = params_dict[\"input_size\"],\n",
    "                                                  hidden_size = params_dict[\"hidden_size\"],\n",
    "                                                  p_hidden = params_dict[\"p_hidden\"],\n",
    "                                                  prep_hidden = params_dict[\"prep_hidden\"],\n",
    "                                                  logvar = params_dict[\"logvar\"],\n",
    "                                                  mixing = params_dict[\"mixing\"],\n",
    "                                                  classification_hidden=params_dict[\"classification_hidden\"],\n",
    "                                                  cov_size = params_dict[\"cov_size\"],\n",
    "                                                  cov_hidden = params_dict[\"cov_hidden\"],\n",
    "                                                  dropout_rate = params_dict[\"dropout_rate\"],\n",
    "                                                  full_gru_ode= params_dict[\"full_gru_ode\"],\n",
    "                                                  impute = params_dict[\"impute\"])\n",
    "\n",
    "    nnfwobj.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(nnfwobj.parameters(), lr=params_dict[\"lr\"], weight_decay= params_dict[\"weight_decay\"])\n",
    "    class_criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    val_metric_prev = -1000\n",
    "    print(\"Start Training\")\n",
    "    for epoch in range(epoch_max):\n",
    "        nnfwobj.train()\n",
    "        total_train_loss = 0\n",
    "        auc_total_train  = 0\n",
    "        tot_loglik_loss = 0\n",
    "        for i, b in enumerate(tqdm.tqdm(dl)):\n",
    "            optimizer.zero_grad()\n",
    "            times    = b[\"times\"]\n",
    "            time_ptr = b[\"time_ptr\"]\n",
    "            X        = b[\"X\"].to(device)\n",
    "            M        = b[\"M\"].to(device)\n",
    "            obs_idx  = b[\"obs_idx\"]\n",
    "            cov      = b[\"cov\"].to(device)\n",
    "            labels = b[\"y\"].to(device)\n",
    "            batch_size = labels.size(0)\n",
    "\n",
    "            h0 = 0# torch.zeros(labels.shape[0], params_dict[\"hidden_size\"]).to(device)\n",
    "            hT, loss, class_pred, mse_loss  = nnfwobj(times, time_ptr, X, M, obs_idx, delta_t=params_dict[\"delta_t\"], T=params_dict[\"T\"], cov = cov)\n",
    "\n",
    "            total_loss = (loss + params_dict[\"lambda\"]*class_criterion(class_pred, labels))/batch_size\n",
    "            total_train_loss += total_loss\n",
    "            tot_loglik_loss +=mse_loss\n",
    "            try:\n",
    "                auc_total_train += roc_auc_score(labels.detach().cpu(),torch.sigmoid(class_pred).detach().cpu())\n",
    "            except ValueError:\n",
    "                if params_dict[\"verbose\"]>=3:\n",
    "                    print(\"Single CLASS ! AUC is erroneous\")\n",
    "                pass\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    " \n",
    "        \n",
    "        info = { 'training_loss' : total_train_loss.detach().cpu().numpy()/(i+1), 'AUC_training' : auc_total_train/(i+1), \"loglik_loss\" :tot_loglik_loss.detach().cpu().numpy()}\n",
    "        \n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag, value, epoch)\n",
    "        print(f\"NegLogLik Loss train : {tot_loglik_loss.detach().cpu().numpy()}\")\n",
    "\n",
    "        data_utils.adjust_learning_rate(optimizer,epoch,params_dict[\"lr\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            nnfwobj.eval()\n",
    "            total_loss_val = 0\n",
    "            auc_total_val = 0\n",
    "            loss_val = 0\n",
    "            mse_val  = 0\n",
    "            corr_val = 0\n",
    "            num_obs = 0\n",
    "            for i, b in enumerate(dl_val): #???? check dl_val\n",
    "                times    = b[\"times\"]\n",
    "                time_ptr = b[\"time_ptr\"]\n",
    "                X        = b[\"X\"].to(device)\n",
    "                M        = b[\"M\"].to(device)\n",
    "                obs_idx  = b[\"obs_idx\"]\n",
    "                cov      = b[\"cov\"].to(device)\n",
    "                labels   = b[\"y\"].to(device)\n",
    "                batch_size = labels.size(0)\n",
    "\n",
    "\n",
    "                if b[\"X_val\"] is not None:\n",
    "                    X_val     = b[\"X_val\"].to(device)\n",
    "                    M_val     = b[\"M_val\"].to(device)\n",
    "\n",
    "                    times_val = b[\"times_val\"]\n",
    "                    times_idx = b[\"index_val\"]\n",
    "\n",
    "                h0 = 0 #torch.zeros(labels.shape[0], params_dict[\"hidden_size\"]).to(device)\n",
    "                hT, loss, class_pred, t_vec, p_vec, h_vec, _, _  = nnfwobj(times, time_ptr, X, M, obs_idx, delta_t=params_dict[\"delta_t\"], T=params_dict[\"T\"], cov=cov, return_path=True)\n",
    "                total_loss = (loss + params_dict[\"lambda\"]*class_criterion(class_pred, labels))/batch_size\n",
    "\n",
    "                try:\n",
    "                    auc_val=roc_auc_score(labels.cpu(),torch.sigmoid(class_pred).cpu())\n",
    "                except ValueError:\n",
    "                    auc_val = 0.5\n",
    "                    if params_dict[\"verbose\"]>=3:\n",
    "                        print(\"Only one class : AUC is erroneous\")\n",
    "                    pass\n",
    "\n",
    "                if params_dict[\"lambda\"]==0:\n",
    "                    t_vec = np.around(t_vec,str(params_dict[\"delta_t\"])[::-1].find('.')).astype(np.float32) #Round floating points error in the time vector.\n",
    "\n",
    "\n",
    "                    \n",
    "                    p_val = data_utils.extract_from_path(t_vec,p_vec,times_val,times_idx)\n",
    "                    m, v = torch.chunk(p_val,2,dim=1)\n",
    "                    last_loss = (data_utils.log_lik_gaussian(X_val,m,v)*M_val).sum()\n",
    "                    mse_loss = (torch.pow(X_val-m,2)*M_val).sum()\n",
    "                    corr_val_loss = data_utils.compute_corr(X_val, m, M_val)\n",
    "\n",
    "                    loss_val += last_loss.cpu().numpy()\n",
    "                    num_obs += M_val.sum().cpu().numpy()\n",
    "                    mse_val += mse_loss.cpu().numpy()\n",
    "                    corr_val += corr_val_loss.cpu().numpy()\n",
    "                else:\n",
    "                    num_obs=1\n",
    "\n",
    "                total_loss_val += total_loss.cpu().detach().numpy()\n",
    "                auc_total_val += auc_val\n",
    "\n",
    "            loss_val /= num_obs\n",
    "            mse_val /=  num_obs\n",
    "            info = { 'validation_loss' : total_loss_val/(i+1), 'AUC_validation' : auc_total_val/(i+1),\n",
    "                     'loglik_loss' : loss_val, 'validation_mse' : mse_val, 'correlation_mean' : np.nanmean(corr_val),\n",
    "                    'correlation_max': np.nanmax(corr_val), 'correlation_min': np.nanmin(corr_val)}\n",
    "            for tag, value in info.items():\n",
    "                logger.scalar_summary(tag, value, epoch)\n",
    "\n",
    "            if params_dict[\"lambda\"]==0:\n",
    "                val_metric = - loss_val\n",
    "            else:\n",
    "                val_metric = auc_total_val/(i+1)\n",
    "\n",
    "            if val_metric > val_metric_prev:\n",
    "                print(f\"New highest validation metric reached ! : {val_metric}\")\n",
    "                print(\"Saving Model\")\n",
    "                torch.save(nnfwobj.state_dict(),f\"./../trained_models/{simulation_name}_MAX.pt\")\n",
    "                val_metric_prev = val_metric\n",
    "                test_loglik, test_auc, test_mse = test_evaluation(nnfwobj, params_dict, class_criterion, device, dl_test)\n",
    "                print(f\"Test loglik loss at epoch {epoch} : {test_loglik}\")\n",
    "                print(f\"Test AUC loss at epoch {epoch} : {test_auc}\")\n",
    "                print(f\"Test MSE loss at epoch{epoch} : {test_mse}\")\n",
    "            else:\n",
    "                if epoch % 10:\n",
    "                    torch.save(nnfwobj.state_dict(),f\"./../trained_models/{simulation_name}.pt\")\n",
    "\n",
    "        print(f\"Total validation loss at epoch {epoch}: {total_loss_val/(i+1)}\")\n",
    "        print(f\"Validation AUC at epoch {epoch}: {auc_total_val/(i+1)}\")\n",
    "        print(f\"Validation loss (loglik) at epoch {epoch}: {loss_val:.5f}. MSE : {mse_val:.5f}. Correlation : {np.nanmean(corr_val):.5f}. Num obs = {num_obs}\")\n",
    "\n",
    "    print(f\"Finished training GRU-ODE for Climate. Saved in ./../trained_models/{simulation_name}\")\n",
    "\n",
    "    # return(info, val_metric_prev, test_loglik, test_auc, test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation(model, params_dict, class_criterion, device, dl_test):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss_test = 0\n",
    "        auc_total_test = 0\n",
    "        loss_test = 0\n",
    "        mse_test  = 0\n",
    "        corr_test = 0\n",
    "        num_obs = 0\n",
    "        for i, b in enumerate(dl_test):\n",
    "            times    = b[\"times\"]\n",
    "            time_ptr = b[\"time_ptr\"]\n",
    "            X        = b[\"X\"].to(device)\n",
    "            M        = b[\"M\"].to(device)\n",
    "            obs_idx  = b[\"obs_idx\"]\n",
    "            cov      = b[\"cov\"].to(device)\n",
    "            labels   = b[\"y\"].to(device)\n",
    "            batch_size = labels.size(0)\n",
    "\n",
    "            if b[\"X_val\"] is not None:\n",
    "                X_val     = b[\"X_val\"].to(device)\n",
    "                M_val     = b[\"M_val\"].to(device)\n",
    "                times_val = b[\"times_val\"]\n",
    "                times_idx = b[\"index_val\"]\n",
    "\n",
    "            h0 = 0 #torch.zeros(labels.shape[0], params_dict[\"hidden_size\"]).to(device)\n",
    "            hT, loss, class_pred, t_vec, p_vec, h_vec, _ , _ = model(times, time_ptr, X, M, obs_idx, delta_t=params_dict[\"delta_t\"], T=params_dict[\"T\"], cov=cov, return_path=True)\n",
    "            total_loss = (loss + params_dict[\"lambda\"]*class_criterion(class_pred, labels))/batch_size\n",
    "\n",
    "            try:\n",
    "                auc_test=roc_auc_score(labels.cpu(),torch.sigmoid(class_pred).cpu())\n",
    "            except ValueError:\n",
    "                if params_dict[\"verbose\"]>=3:\n",
    "                    print(\"Only one class. AUC is wrong\")\n",
    "                auc_test = 0\n",
    "                pass\n",
    "\n",
    "            if params_dict[\"lambda\"]==0:\n",
    "                t_vec = np.around(t_vec,str(params_dict[\"delta_t\"])[::-1].find('.')).astype(np.float32) #Round floating points error in the time vector.\n",
    "                p_val = data_utils.extract_from_path(t_vec,p_vec,times_val,times_idx)\n",
    "                m, v = torch.chunk(p_val,2,dim=1)\n",
    "                last_loss = (data_utils.log_lik_gaussian(X_val,m,v)*M_val).sum()\n",
    "                mse_loss = (torch.pow(X_val-m,2)*M_val).sum()\n",
    "                corr_test_loss = data_utils.compute_corr(X_val, m, M_val)\n",
    "\n",
    "                loss_test += last_loss.cpu().numpy()\n",
    "                num_obs += M_val.sum().cpu().numpy()\n",
    "                mse_test += mse_loss.cpu().numpy()\n",
    "                corr_test += corr_test_loss.cpu().numpy()\n",
    "            else:\n",
    "                num_obs=1\n",
    "\n",
    "            total_loss_test += total_loss.cpu().detach().numpy()\n",
    "            auc_total_test += auc_test\n",
    "\n",
    "        loss_test /= num_obs\n",
    "        mse_test /=  num_obs\n",
    "        auc_total_test /= (i+1)\n",
    "\n",
    "        return(loss_test, auc_total_test, mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [02:03<02:04, 41.50s/it]"
     ]
    }
   ],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "\n",
    "    simulation_name=\"me\"\n",
    "    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    train_idx = np.load(\"../../gru_ode_bayes/datasets/me/datasets/me_0/train_idx.npy\",allow_pickle=True)\n",
    "    val_idx = np.load(\"../../gru_ode_bayes/datasets/me/datasets/me_0/val_idx.npy\",allow_pickle=True)\n",
    "    test_idx = np.load(\"../../gru_ode_bayes/datasets/me/datasets/me_0/test_idx.npy\",allow_pickle=True)\n",
    "\n",
    "    #Model parameters.\n",
    "    params_dict=dict()\n",
    "    params_dict[\"csv_file_path\"] = \"../../gru_ode_bayes/datasets/me/me.csv\"\n",
    "    params_dict[\"csv_file_tags\"] = None\n",
    "    params_dict[\"csv_file_cov\"]  = None\n",
    "\n",
    "\n",
    "    params_dict[\"T\"]=2020 # Time that want to predict, at least = max time in data + 1\n",
    "    params_dict[\"T_val\"] = 2000 # Time that get observation\n",
    "    params_dict[\"max_val_samples\"] = 3 # Numbers of multi predict\n",
    "\n",
    "\n",
    "    # train_idx = np.load(\"../../gru_ode_bayes/datasets/Climate/datasets/climate/train_idx.npy\",allow_pickle=True)\n",
    "    # val_idx = np.load(\"../../gru_ode_bayes/datasets/Climate/datasets/climate/val_idx.npy\",allow_pickle=True)\n",
    "    # test_idx = np.load(\"../../gru_ode_bayes/datasets/Climate/datasets/climate/test_idx.npy\",allow_pickle=True)\n",
    "    #\n",
    "    # #Model parameters.\n",
    "    # params_dict=dict()\n",
    "    # params_dict[\"csv_file_path\"] = \"../../gru_ode_bayes/datasets/Climate/climate.csv\"\n",
    "    # params_dict[\"csv_file_tags\"] = None\n",
    "    # params_dict[\"csv_file_cov\"]  = None\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    params_dict[\"hidden_size\"] = 50\n",
    "    params_dict[\"p_hidden\"] = 25\n",
    "    params_dict[\"prep_hidden\"] = 10\n",
    "    params_dict[\"logvar\"] = True\n",
    "    params_dict[\"mixing\"] = 1e-4 #Weighting between KL loss and MSE loss.\n",
    "    params_dict[\"delta_t\"]=0.1\n",
    "\n",
    "\n",
    "    params_dict[\"lambda\"] = 0 #Weighting between classification and MSE loss.\n",
    "    params_dict[\"classification_hidden\"] = 2\n",
    "    params_dict[\"cov_hidden\"] = 50\n",
    "    params_dict[\"weight_decay\"] = 0.0001\n",
    "    params_dict[\"dropout_rate\"] = 0.2\n",
    "    params_dict[\"lr\"]=0.001\n",
    "    params_dict[\"full_gru_ode\"] = True\n",
    "    params_dict[\"no_cov\"] = True\n",
    "    params_dict[\"impute\"] = False\n",
    "    params_dict[\"verbose\"] = 0 #from 0 to 3 (highest)\n",
    "\n",
    "\n",
    "\n",
    "    train_gruode (\n",
    "                    simulation_name = simulation_name,\n",
    "                    params_dict = params_dict,\n",
    "                    device = device,\n",
    "                    train_idx = train_idx,\n",
    "                    val_idx = val_idx,\n",
    "                    test_idx = test_idx,\n",
    "                    epoch_max=1\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
